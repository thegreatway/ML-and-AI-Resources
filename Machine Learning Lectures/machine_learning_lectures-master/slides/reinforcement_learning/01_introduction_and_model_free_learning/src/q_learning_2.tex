\bgroup
\begin{frame}{Off-policy control with Q-learning}
\begin{itemize}
\item We now allow both behaviour and target policies to \highlight{improve}
\item The target policy $\pi$ is \highlight{greedy} w.r.t. $Q(s, a)$
\begin{equation*}
\pi(S_{t+1}) = \argmax_{a^{\prime}}Q(S_{t+1}, a^{\prime})
\end{equation*}
\item The behaviour policy $\mu$ is \highlight{$\epsilon$-greedy} w.r.t. $Q(s, a)$
\item The Q-learning target then simplifies:
\begin{align*}
& R_{t+1} + \gamma Q(S_{t+1}, A^{\prime})\\
=& R_{t+1} + \gamma Q(S_{t+1}, \argmax_{a^{\prime}} Q(S_{t+1}, a^{\prime}))\\
=& R_{t+1} + \gamma \max_{a^{\prime}}Q(S_{t+1},a^{\prime})
\end{align*}
\end{itemize}
\end{frame}
\egroup