\bgroup
\begin{frame}{Bellman Expectation Equation}
The state-value function can again be decomposed into immediate reward plus discounted value of successor state,
\begin{equation*}
v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
\end{equation*}
The action-value function can similarly be decomposed,
\begin{equation*}
q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
\end{equation*}
\end{frame}
\egroup